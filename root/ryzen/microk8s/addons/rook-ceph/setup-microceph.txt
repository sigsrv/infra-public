sudo snap install microceph
sudo snap connect microceph:dm-crypt
sudo snap restart microceph.daemon

sudo microceph init

```
Please choose the address MicroCeph will be listening on [default=175.196.126.252]: 10.10.10.10
Would you like to create a new MicroCeph cluster? (yes/no) [default=no]: yes
Please choose a name for this system [default=sigsrv]:
Would you like to add additional servers to the cluster? (yes/no) [default=no]:
Would you like to add additional local disks to MicroCeph? (yes/no) [default=yes]:

Available unpartitioned disks on this system:
+---------------+----------+------+-----------------------------------------------------------+
|     MODEL     | CAPACITY | TYPE |                           PATH                            |
+---------------+----------+------+-----------------------------------------------------------+
| SHPP41-2000GM | 1.82TiB  | nvme | /dev/disk/by-id/nvme-eui.ace42e0035e87a292ee4ac0000000001 |
+---------------+----------+------+-----------------------------------------------------------+
| SHPP41-2000GM | 1.82TiB  | nvme | /dev/disk/by-id/nvme-eui.ace42e0035e87a312ee4ac0000000001 |
+---------------+----------+------+-----------------------------------------------------------+
What's the disk path? (empty to exit): /dev/disk/by-id/nvme-eui.ace42e0035e87a292ee4ac0000000001
Would you like the disk to be wiped? [default=no]: yes
Would you like the disk to be encrypted? [default=no]: yes
What's the disk path? (empty to exit): /dev/disk/by-id/nvme-eui.ace42e0035e87a312ee4ac0000000001
Would you like the disk to be wiped? [default=no]: yes
Would you like the disk to be encrypted? [default=no]: yes
What's the disk path? (empty to exit):
```

sudo microceph.ceph osd crush rule rm replicated_rule
sudo microceph.ceph osd crush rule create-replicated single default osd
sudo microceph.ceph config set global osd_pool_default_size 2
sudo microceph.ceph config set mgr mgr_standby_modules false
sudo microceph.ceph config set osd osd_crush_chooseleaf_type 0
sudo microceph.ceph config set global mon_max_pg_per_osd 512

sudo microceph enable mds
sudo microceph enable mgr
sudo microceph enable mon
sudo microceph enable rgw --port 8080

sudo ceph mgr module enable prometheus

for pool in (sudo ceph osd lspools | awk '{print $2}')
    sudo ceph osd pool set $pool size 2
end

sudo ceph osd pool create sigsrv.lxd.osd.meta
sudo ceph osd pool create sigsrv.lxd.osd.data
sudo ceph osd pool set sigsrv.lxd.osd.meta pg_autoscale_mode on
sudo ceph osd pool set sigsrv.lxd.osd.data pg_autoscale_mode on
sudo ceph osd pool application enable sigsrv.lxd.osd.meta rbd
sudo ceph osd pool application enable sigsrv.lxd.osd.data rbd

lxc storage create microceph-lxd ceph \
    ceph.osd.pool_name=sigsrv.lxd.osd.meta \
    ceph.osd.data_pool_name=sigsrv.lxd.osd.data

sudo ceph osd pool create sigsrv.microk8s.cephfs.meta
sudo ceph osd pool create sigsrv.microk8s.cephfs.data
sudo ceph osd pool set sigsrv.microk8s.cephfs.meta pg_autoscale_mode on
sudo ceph osd pool set sigsrv.microk8s.cephfs.data pg_autoscale_mode on

sudo ceph fs new sigsrv.microk8s.cephfs \
    sigsrv.microk8s.cephfs.meta \
    sigsrv.microk8s.cephfs.data

sudo ceph osd pool create sigsrv.microk8s.rbd.data
sudo ceph osd pool set sigsrv.microk8s.rbd.data pg_autoscale_mode on
